<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>图像自动生成描述 - Masen Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Masen Blog"><meta name="msapplication-TileImage" content="/img/favicon.JPG"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Masen Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="总结于：http:&amp;#x2F;&amp;#x2F;cs.stanford.edu&amp;#x2F;people&amp;#x2F;karpathy&amp;#x2F;main.pdf  1. 目的长期目标：计算机理解知识可以总结为通过两个渠道：（1）通过物理媒介：基于传感器和现实世界的物体及交互；（2）通过互联网：主要通过互联网的自然语言。研究图像描述生成即是研究视觉和自然语言的联系，通过构建计算机对视觉和语言的联系，可以构建更加智能的系统，例如根据网络语言学习新知识、理解"><meta property="og:type" content="blog"><meta property="og:title" content="图像自动生成描述"><meta property="og:url" content="http://leewangml.com/2020/01/17/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%BB%BC%E8%BF%B0/"><meta property="og:site_name" content="Masen Blog"><meta property="og:description" content="总结于：http:&amp;#x2F;&amp;#x2F;cs.stanford.edu&amp;#x2F;people&amp;#x2F;karpathy&amp;#x2F;main.pdf  1. 目的长期目标：计算机理解知识可以总结为通过两个渠道：（1）通过物理媒介：基于传感器和现实世界的物体及交互；（2）通过互联网：主要通过互联网的自然语言。研究图像描述生成即是研究视觉和自然语言的联系，通过构建计算机对视觉和语言的联系，可以构建更加智能的系统，例如根据网络语言学习新知识、理解"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://leewangml.oss-cn-beijing.aliyuncs.com/images/20191224191812.png"><meta property="og:image" content="https://leewangml.oss-cn-beijing.aliyuncs.com/images/20191225171945.png"><meta property="og:image" content="https://leewangml.oss-cn-beijing.aliyuncs.com/images/20191227110528.png"><meta property="og:image" content="https://leewangml.oss-cn-beijing.aliyuncs.com/images/20200117082649.png"><meta property="article:published_time" content="2020-01-17T18:26:00.000Z"><meta property="article:modified_time" content="2022-08-03T22:18:23.257Z"><meta property="article:author" content="Zhe Wang"><meta property="article:tag" content="自然语言处理"><meta property="article:tag" content="图像处理"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://leewangml.oss-cn-beijing.aliyuncs.com/images/20191224191812.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://leewangml.com/2020/01/17/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%BB%BC%E8%BF%B0/"},"headline":"图像自动生成描述","image":["https://leewangml.oss-cn-beijing.aliyuncs.com/images/20191224191812.png","https://leewangml.oss-cn-beijing.aliyuncs.com/images/20191225171945.png","https://leewangml.oss-cn-beijing.aliyuncs.com/images/20191227110528.png","https://leewangml.oss-cn-beijing.aliyuncs.com/images/20200117082649.png"],"datePublished":"2020-01-17T18:26:00.000Z","dateModified":"2022-08-03T22:18:23.257Z","author":{"@type":"Person","name":"Zhe Wang"},"publisher":{"@type":"Organization","name":"Masen Blog","logo":{"@type":"ImageObject","url":"http://leewangml.com/img/logo.svg"}},"description":"总结于：http:&#x2F;&#x2F;cs.stanford.edu&#x2F;people&#x2F;karpathy&#x2F;main.pdf  1. 目的长期目标：计算机理解知识可以总结为通过两个渠道：（1）通过物理媒介：基于传感器和现实世界的物体及交互；（2）通过互联网：主要通过互联网的自然语言。研究图像描述生成即是研究视觉和自然语言的联系，通过构建计算机对视觉和语言的联系，可以构建更加智能的系统，例如根据网络语言学习新知识、理解"}</script><link rel="canonical" href="http://leewangml.com/2020/01/17/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%BB%BC%E8%BF%B0/"><link rel="icon" href="/img/favicon.JPG"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }
          Array
              .from(document.querySelectorAll('.tab-content'))
              .forEach($tab => {
                  $tab.classList.add('is-hidden');
              });
          Array
              .from(document.querySelectorAll('.tabs li'))
              .forEach($tab => {
                  $tab.classList.remove('is-active');
              });
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.0.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Masen Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="hhttps://github.com/Mr-Wang119"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-01-17T18:26:00.000Z" title="1/17/2020, 10:26:00 AM">2020-01-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-08-03T22:18:23.257Z" title="8/3/2022, 3:18:23 PM">2022-08-03</time></span><span class="level-item"><a class="link-muted" href="/categories/%E5%9B%BE%E5%83%8F/">图像</a></span><span class="level-item">22 minutes read (About 3232 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">图像自动生成描述</h1><div class="content"><blockquote>
<p>总结于：<a target="_blank" rel="noopener" href="http://cs.stanford.edu/people/karpathy/main.pdf">http://cs.stanford.edu/people/karpathy/main.pdf</a></p>
</blockquote>
<h2 id="1-目的"><a href="#1-目的" class="headerlink" title="1. 目的"></a>1. 目的</h2><p><strong>长期目标</strong>：计算机理解知识可以总结为通过两个渠道：（1）通过物理媒介：基于传感器和现实世界的物体及交互；（2）通过互联网：主要通过互联网的自然语言。研究图像描述生成即是研究视觉和自然语言的联系，通过构建计算机对视觉和语言的联系，可以构建更加智能的系统，例如根据网络语言学习新知识、理解知识并在现实生活中作出反应。</p>
<p><strong>短期目标</strong>：（1）由于复杂的序列语言可以表示名词、动词、形容词等，因此可以泛化目前一系列相对独立的cv领域，例如目标分类、动作识别等；（2）由于用户对自然语言掌握程度比独立词语更好，因此更符合用户需要，例如用户在搜寻照片时，通过一段文字来搜寻比通过关键字搜索更加符合用户习惯。</p>
<span id="more"></span>

<h2 id="2-挑战"><a href="#2-挑战" class="headerlink" title="2. 挑战"></a>2. 挑战</h2><ul>
<li>评估困难：在图像分类任务中，可以通过比较分类结果的正确性来达到评估目的，但是很难根据得到的自然语言语句结果得到精确的评估结果。通过使用一些最先进的自动评估方法，可以达到和人类评估类似的评估结果；</li>
<li>模型分离：将视觉识别任务和语言建模任务结合在一起的研究中，很容易将其分解为两个部分进行单独考虑，即图像处理和自然语言生成两个部分，这与传统模型类似，只是做了合并。可以通过端到端的方式进行训练，仅通过单独的模型达到目的，而不需要对两个方面进行单独的考虑。</li>
</ul>
<h2 id="3-相关工作"><a href="#3-相关工作" class="headerlink" title="3. 相关工作"></a>3. 相关工作</h2><h2 id="4-数据源"><a href="#4-数据源" class="headerlink" title="4. 数据源"></a>4. 数据源</h2><blockquote>
<p> 链接:<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1YNGB3ANAwxOsv34NscJWcQ">https://pan.baidu.com/s/1YNGB3ANAwxOsv34NscJWcQ</a>  密码:502r</p>
<p>数据集具体介绍：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.06475">https://arxiv.org/abs/1711.06475</a></p>
</blockquote>
<p>数据源为AI Challenger全球AI挑战赛数据集，数据集中包含30万张图片和150万句中文描述，其中每张图片对应5句中文描述。</p>
<blockquote>
<h3 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h3><p><strong>caption 和 description 的区别？</strong></p>
<p>caption：主要关注图片外的隐性信息，如拍摄时间、地点、状态等；</p>
<p>description：主要关注图片本身内容的描述。</p>
</blockquote>
<h2 id="5-实现过程"><a href="#5-实现过程" class="headerlink" title="5. 实现过程"></a>5. 实现过程</h2><h3 id="1-模型介绍"><a href="#1-模型介绍" class="headerlink" title="(1) 模型介绍"></a>(1) 模型介绍</h3><p>使用 Multimodal RNN language model 来实现生成图片描述。在图片描述生成任务中，首先使用CNN提取图像特征，然后将图像特征和每一步预测的当前词送入RNN结构中，即下图中的结构：</p>
<p><img src="https://leewangml.oss-cn-beijing.aliyuncs.com/images/20191224191812.png"></p>
<p>我们可以用下面的公式表示上图的关系：<br>$$ b_V &#x3D; W_{hi}[CNN_{\theta_c}(I)], $$<br>$$ h_t &#x3D; f(W_{hx}x_t+W_{hh}h_{t-1}+b_h+\delta{t&#x3D;1}\bigodot b_v), $$<br>$$ y_t&#x3D;W_{oh}h_t+b_o. $$</p>
<p>其中, W、$ b_h $、$ b_o $为要学习的参数，$ CNN_{\theta_c}(I) $为CNN的最后一个隐层输出（全连接层前一层），$ \delta{t&#x3D;1}\bigodot b_v $为delta表达式，表示仅当t&#x3D;1时带入$ b_v $计算（这里的原因是原文实验验证后得到，仅把图像内容作为RNN的第一层输入效果比将图像作为RNN每层输入效果好）。因此RNN中在每一步中传递的信息为图像信息和文本信息，要实现这一效果，我们需要证明虽然图像信息仅在RNN第一步传入，但可以将其传递给后面每一步。当$ W_{hh} $为[1, 0, 0, …]、且$ W_{hx} $的第一行为全0时，可以将完整的传递图像信息传递给下一层。在RNN中，第一步我们可以传入指定的START词向量，在生成完成后回得到END词向量。在反向传播的过程中，为了方便并行运算，可以让RNN在生成END词向量后的每一个时间步都生成END。在测试过程中，可以使用beam search方法提升算法的准确度，生成更关注图像全局信息的语句。</p>
<h3 id="2-模型优化"><a href="#2-模型优化" class="headerlink" title="(2) 模型优化"></a>(2) 模型优化</h3><p>在训练过程中我们会发现模型较难优化，这是因为不同词语之间的频率差异过于悬殊，例如END会出现在每一个句子中，而牙刷可能在训练集中一共出现五次。为了缓解数据集中数据数量差异问题，我们可以使用RMSprop、SGD、SGD+Momentum、Adam等方式对参数更新进行优化。实验验证RMSprop和Adam效果较好。当在训练前显试初始化字典中所有词的偏向，可以带来较快的收敛速度。实验结果也证明，使用word2vec向量初始化单词向量和随机初始化单词向量达到的效果基本类似。</p>
<h3 id="3-评价标准"><a href="#3-评价标准" class="headerlink" title="(3) 评价标准"></a>(3) 评价标准</h3><p>生成文本的评价标准使用BLEU score，BLEU用于评价机器翻译的充分性和流畅度。其中需要指定n-gram，即比较生成文本中长度为n的文本片段和参考文本中的长度为n的文本片段的匹配片段的个数，匹配片段个数越多，生成文本效果越好。BLEU的计算公式使用Brevity Penalty值评价翻译的完整度，公式如下，其中c为待评价语句长度，r为参考语句长度：</p>

$$
BP=\begin{cases}

1, \quad c>r \\[2ex]

e^{(1-r/c)}, \quad c \leq r.

\end{cases}
$$

BLEU值的计算公式为：

$$
BLEU=BP\cdot exp(\sum_{n=1}^{N}w_nlogp_n).
$$

<p>BLEU值越大越好。</p>
<h2 id="6-一些改进"><a href="#6-一些改进" class="headerlink" title="6. 一些改进"></a>6. 一些改进</h2><h3 id="1-模型介绍-1"><a href="#1-模型介绍-1" class="headerlink" title="(1)模型介绍"></a>(1)模型介绍</h3><p>在上一节，使用了Multimodal RNN实现了对图像描述的生成，但这个方面有一定的局限性：对于比较复杂的图像，上面的结构仅能生成一句较为粗略的描述，我们可以通过改进网络结构生成多句对同一图像不同关注点的描述，下图为文献中一个较为形象的解释：</p>
<p><img src="https://leewangml.oss-cn-beijing.aliyuncs.com/images/20191225171945.png"></p>
<p>在几年前，研究人员较好地实现了对于图像的单标签分类。在近几年，研究领域主要集中在两个方面：（1）检测单张图片的多个物体区域并分类；（2）增加生成标签语句的复杂度，使其较为完整地描述图像内容。然而这两个研究方向处于孤立的状态，下面我们将介绍如何实现对于复杂图片的密集描述模型，该模型的示意图如下图所示：</p>
<p><img src="https://leewangml.oss-cn-beijing.aliyuncs.com/images/20191227110528.png"></p>
<p>网络以图像为输入，通过CNN生成图像的多个特征数据。这些特征数据通过Localization Layer提取特征区域并使用双线性插值作为激活函数，之后通过RNN生成连贯的语句。</p>
<h3 id="2-模型结构"><a href="#2-模型结构" class="headerlink" title="(2) 模型结构"></a>(2) 模型结构</h3><ul>
<li><strong>卷积网络</strong></li>
</ul>
<p>使用VGG-16，共有13层3x3卷积和5层2x2最大值池化，在实验中去掉了最后一层池化，因此输入的图像数据为$ 3\times W\times H $，输出的特征数据维度为$ C\times W’\times H’ $，其中$ C&#x3D;512, W’&#x3D;\lfloor \frac{W}{16} \rfloor, H’&#x3D;\frac{H}{16} $. 这些数据被送入Localization Layer进行下一步处理。</p>
<ul>
<li><strong>全卷积Localization Layer</strong></li>
</ul>
<p>Localization Layer 以图像的特征数据为输入，得到图片中的兴趣空间区域并将这些空间区域以定长的特征表示出来。该层模型以Faster R-CNN为基础，并将RoI采样更换为双线性差值，使得模型可以利用生成的区域坐标反向传播更新参数。这一修改也使得兴趣空间区域不一定是矩形，可以是多边形。</p>
<p><strong>输入&#x2F;输出：</strong></p>
<p>Localization Layer接受大小为$ C \times W’\times H’ $，然后会选出其中B个兴趣区域，之后计算并返回结果，返回结果包括：</p>
<p>​    (1) <strong>区域坐标</strong>：同一图像的一组区域坐标，维度为$ B\times 4 $, 分别为中心点$ (x_a, y_a) $，宽$ w_a $和高$ h_a $；</p>
<p>​    (2) <strong>区域得分</strong>：图片区域的得分向量，具有高置信度分数的区域更可能对应于真实的感兴趣区域，维度为$ B\times 1 $;</p>
<p>​    (3) <strong>区域特征</strong>：各个区域图像特征的矩阵，大小为$ B\times C\times X\times Y $，因此每一个区域被表示为在$ X\times Y $区域下的C维特征，实验中$ X, Y, C $分别为(7, 7, 512)；</p>
<p><strong>卷积锚：</strong></p>
<p>Localization Layer不是直接从原图像上寻找候选区域，而是通过在缩放后的信息中生成图像区域矩形框的位置和为物体的置信得分。在Localization Layer中，针对每幅图像，遍历各个锚点，并使用计算k个不同宽高比（类似于多尺度方法）的锚框的置信得分。模型中将输入的特征传入有256个滤波器的3x3卷积中，之后通过一个非线性层和有5k个滤波器的1x1卷积。因此输出为$ 5k\times W’ \times H’ $。其中，一个通道 $ 5k\times W’ \times H’$表示锚框的置信得分，其余4个通道为用于更新卷积锚的偏移量。</p>
<p><strong>边框采样：</strong></p>
<p>在选择卷积候选区域的过程中我们使用了特征图上每一个关键点的k个不同大小的候选区域，因此对于大小为$ W’\times H’ $的特征图像，其生成置信得分的维度为$ k\times W’\times H’ $。由于这个数量较大，因此考虑对边框进行采样。采样的方式在训练和测试过程中有所不同。</p>
<p>在训练中，我们共采样B&#x3D;256个边框，其中最多1&#x2F;2为正向区域（Positive regions），其余为负向区域（Negative regions）。当一个候选区域置信得分的Intersection over Union (IoU) 值大于等于0.7，它就被称为一个正向区域；相反，当其IoU值小于0.3，其为一个负向区域。在实验中，采样的小批梯度采样包括$ B_P&lt;B&#x2F;2 $的正向区域和$ B_N&#x3D;B-B_P $的负向区域。</p>
<p>在测试过程中，我们使用非极大值抑制（NMS）来选择$ B&#x3D;300 $个得分最高的边框。NMS的原理是通过对所有边框得分进行排序，保留最高的分数，而与得分最高的边框重合面积（IOU）超过阈值的边框则删除。使用NMS并不是一个好的方法，因为在训练中和测试中使用了不同的采样方式，但是设计一个NMS神经网络或者使用其他通用的可微采样方法更为困难。</p>
<p><strong>边框回归：</strong></p>
<p>在上文我们求出了卷积锚，之后我们还需要将卷积锚还原为候选区域，给定锚框的中心$ (X_a, y_a) $，宽$ w_a $和高$ h_a $，模型可以输出归一化后的偏移量和对数空间的变化量$ (t_x, t_y, t_w, t_h) $，因此输出区域的中心$ (x, y) $和长宽$ (w, h) $的计算公式为：</p>

$$
x=x_a+t_xw_a\\
y=y_a+t_yh_a\\
w=w_aexp(t_w)\\
h=h_aexp(h_w).
$$

<p>当模型输出全0时，锚框坐标及大小完全等于候选区域的坐标和大小，符合实际情况。</p>
<p><strong>双线性插值：</strong></p>
<p>经过采样后，我们得到了宽高比不同的多个候选区域，为了下一步送入识别网络和RNN，需要统一这些候选区域数据的维度。</p>
<p>Fast R-CNN使用了RoI池化层，将$ W’\times H’ $的候选区域分割为$ X\times Y$个网格，然后利用最大层池化得到$ X\times Y $的输出特征。这个方法的输入为卷积特征和候选区域的坐标，但是其在训练时只能更新卷积特征，无法更新候选区域的坐标，因此对这个方面进行了改进。</p>
<p>在论文中，使用了双线性插值来生成$ X\times Y $的输出特征数据，以实现对卷积特征和候选区域坐标的同时更新。我们的目的是将维度为$ C\times W’\times H’ $的输入特征$ U $生成输出特征映射$V$，其维度为$ C\times X\times Y $。双线性插值分为一下 个步骤：</p>
<p>（1）计算输出特征$V$的每一个坐标点在输入特征的映射坐标点。这里计算的原因是双线性插值需要计算映射坐标到周围坐标的距离。这里计算得到的坐标不一定是整数，因此并不是实际的坐标，只是用于中间计算。假设我们正在计算输出特征$V$中一点$(c,x_{i,j}^v,y_{i,j}^v)$到输入特征$U$的映射点$(c,x_{i,j},y_{i,j})$的值，其公式为：</p>

$$
x_{i,j}=\frac{x_{i,j}^v}{X}\cdot W'\\
y_{i,j}=\frac{y_{i,j}^v}{Y}\cdot H'.
$$

<p>得到映射点坐标后，设双线性采样的核函数为$k$，使用下面的公式我们就可以计算输入特征$U$到输出特征$V$的特征映射：</p>

$$
V_{c,i,j}=\sum_{i'=1}^W'\sum_{j'=1}^{H'}U_{c,i',j'}k(i'-x_{i,j})k(j'-y_{i,j})\\
k(d)=max(0,1-|d|).
$$

<p>实际计算时我们会发现，参与计算的输入特征的特征点为计算的到的映射点坐标周围坐标值。计算后我们就可以得到一个$B\times C\times X\times Y$的输出，这个输出特征即为Localization Layer的输出。</p>
<ul>
<li><strong>识别网络</strong></li>
</ul>
<p>识别网络是一个全连接网络，处理Localization Layer的输出，再最后一次调整并筛选候选区域，生成$B\times D$的矩阵作为RNN的输入，其中$D&#x3D;4096$，为每一个最终候选区域特征的向量表示维度。识别网络会调整候选区域的位置及得分，使用类似边框回归的方式更新候选区域位置。</p>
<ul>
<li><strong>RNN语言模型</strong></li>
</ul>
<p>使用LSTM实现文本的生成。在训练过程中，我们使用$T+2$个的词向量进行训练，即$x_{-1},x_0,x_1,…,x_T$。其中，$x_{-1}$表示候选区域特征的编码结果，在识别网络中，我们得到了各个候选区域的特征表示，之后通过一个linear layer和ReLU非线性函数，可以得到候选区域所有特征的编码结果。$x_0$为特殊的START编码标识。$x_1,…,x_T$编码了描述当前图片的每个词的词向量，以及最终的特殊END标识。</p>
<p>在测试时我们使用图像特征$x_{-1}$和START特殊标识作为输入，之后在每一个时间步生成下一个最可能的单词，之后送入RNN中继续下一个时间步，直到生成END标识。</p>
<h3 id="3-损失函数"><a href="#3-损失函数" class="headerlink" title="(3) 损失函数"></a>(3) 损失函数</h3><p>损失函数由五项组成。在Localization Layer和识别网络中，都计算了候选区域的坐标和得分，这两个地方都使用了binary logistic losses更新得分，使用smooth L1 loss更新候选区域坐标。最后，在RNN的每一个时间步中使用cross-entropy更新RNN参数。所有的损失函数都依照批的大小和RNN中序列的长度进行了归一化。</p>
<p><strong>Smooth L1 loss:</strong></p>

$$
Smooth\ L1=\begin{cases}
0.5x^2,&|x|<1\\
|x|-0.5,&x<-1\ or\ x>1
\end{cases}
$$
$$
Smooth\ L_1'=\begin{cases}
x,&|x|<1\\
-1,&x<-1\\
1,&x>1
\end{cases}
$$

<p>在边框回归预测时，通常选用L2损失，即平方损失函数。但当离群点存在时，这些点会占据loss的主要部分，为了解决这个问题，FastRCNN使用Smooth L1 loss。相比L2 loss，Smooth L1 loss对离群点更加鲁棒，它随着误差线性增长而不是平方增长。下图可以直观感受两者的函数梯度的区别：</p>
<img src="https://leewangml.oss-cn-beijing.aliyuncs.com/images/20200117082649.png" style="zoom:50%;" />

<blockquote>
<p>参考资料及图片来源：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/48426076">https://zhuanlan.zhihu.com/p/48426076</a></p>
</blockquote>
<h3 id="4-训练和优化"><a href="#4-训练和优化" class="headerlink" title="(4) 训练和优化"></a>(4) 训练和优化</h3><p>由于是端到端的模型，因此每轮更新一次所有参数。模型中的CNN使用了ImageNet的预训练网络，其余网络使用标准差为0.01的高斯分布生成。在训练时，使用动量为0.9的随机梯度下降更新CNN的权重，使用Adam更新网络其他部分的权重。模型使用初始学习率$ 1\times10^{-6} $，设置$ \beta_1&#x3D;0.9,\beta_2&#x3D;0.99 $。在CNN训练一轮后进行fine-tuning。</p>
<p>用于训练的图像数据预先进行过剪裁，最长边为720像素。原文提到一个小批量在Titan X约运行300ms，总共花了3天训练达到模型收敛。</p>
<h3 id="5-实验"><a href="#5-实验" class="headerlink" title="(5) 实验"></a>(5) 实验</h3><p>在实验中，只记录一些关于数据预处理和评价标准的内容。</p>
<ul>
<li><strong>数据预处理</strong></li>
</ul>
<p>在数据预处理过程中，首先统计单词的总出现次数。总出现次数少于15的会被更换为<UNK>标签。之后，去掉了描述中类似“there is…”或“this seems to be a…”。出于训练效率考虑，删除了超过10个词的描述。同时删除了描述少于20句或多于50句的图片，以减小每个图片的区域数量方差。</p>
<ul>
<li><strong>评价标准</strong></li>
</ul>
<p>Localization Layer：IoU</p>
<p>语言：METEOR</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>图像自动生成描述</p><p><a href="http://leewangml.com/2020/01/17/图像描述自动生成综述/">http://leewangml.com/2020/01/17/图像描述自动生成综述/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Zhe Wang</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2020-01-17</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2022-08-03</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a><a class="link-muted mr-2" rel="tag" href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/">图像处理</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/01/22/Sequence%20Models%20%E7%BB%BC%E8%BF%B0/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">GRU LSTM 综述</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2019/10/26/%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90%E6%96%87%E6%9C%AC%E6%8F%8F%E8%BF%B0/"><span class="level-item">图片生成文本描述</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "75a19c8697e204c06dca2badb4d0131f",
            repo: "https://github.com/Mr-Wang119/Mr-Wang119.github.io",
            owner: "Mr-Wang119",
            clientID: "7ba4a13c098768702c7c",
            clientSecret: "b09b3b9b2aed35c372561f49a8b8cf26e9926204",
            admin: ["Mr-Wang119"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 20,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            language: "en",
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://s3.amazonaws.com/images.masen.com/IMG_3439.JPG" alt="Zhe (Masen) Wang"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Zhe (Masen) Wang</p><p class="is-size-6 is-block">ECE M.Eng. UIUC</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Urbana,IL,USA</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">7</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">3</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">10</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Mr-Wang119" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Mr-Wang119"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Linkedin" href="https://www.linkedin.com/in/zhe-joe-wang-79b1b31a4/"><i class="fab fa-linkedin"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="mailto:zhewang8@illinois.edu"><i class="fa fa-envelope"></i></a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/%E5%9B%BE%E5%83%8F/"><span class="level-start"><span class="level-item">图像</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"><span class="level-start"><span class="level-item">机器学习基础</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"><span class="level-start"><span class="level-item">自然语言处理</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-03-12T18:55:54.000Z">2022-03-12</time></p><p class="title"><a href="/2022/03/12/Learning-Notes-Google-File-System/">Learning Notes: Google File System</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-02-19T18:05:51.000Z">2022-02-19</time></p><p class="title"><a href="/2022/02/19/Introduction-of-Eluerian-path/">Introduction of Eulerian path</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-01-22T19:42:24.000Z">2020-01-22</time></p><p class="title"><a href="/2020/01/22/Sequence%20Models%20%E7%BB%BC%E8%BF%B0/">GRU LSTM 综述</a></p><p class="categories"><a href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-01-17T18:26:00.000Z">2020-01-17</time></p><p class="title"><a href="/2020/01/17/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%BB%BC%E8%BF%B0/">图像自动生成描述</a></p><p class="categories"><a href="/categories/%E5%9B%BE%E5%83%8F/">图像</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2019-10-27T06:23:19.000Z">2019-10-26</time></p><p class="title"><a href="/2019/10/26/%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90%E6%96%87%E6%9C%AC%E6%8F%8F%E8%BF%B0/">图片生成文本描述</a></p><p class="categories"><a href="/categories/%E5%9B%BE%E5%83%8F/">图像</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/03/"><span class="level-start"><span class="level-item">March 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/02/"><span class="level-start"><span class="level-item">February 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/01/"><span class="level-start"><span class="level-item">January 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">October 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">April 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Algorithms/"><span class="tag">Algorithms</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Distributed-System/"><span class="tag">Distributed System</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"><span class="tag">图像处理</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"><span class="tag">图像特征提取</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AB%9E%E8%B5%9B/"><span class="tag">竞赛</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"><span class="tag">自然语言处理</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"><span class="tag">语言模型</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"><span class="tag">集成学习</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9D%A2%E8%AF%95/"><span class="tag">面试</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%A1%B9%E7%9B%AE/"><span class="tag">项目</span><span class="tag">1</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Masen Blog" height="28"></a><p class="is-size-7"><span>&copy; 2022 Zhe Wang</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Mr-Wang119"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>